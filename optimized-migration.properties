# Optimized Migration Properties for Large Datasets
# This file addresses the "too many clients already" error and optimizes performance

# =============================================================================
# CASSANDRA CONNECTION SETTINGS
# =============================================================================
spark.cdm.connect.origin.host=your_cassandra_host
spark.cdm.connect.origin.port=9042
spark.cdm.connect.origin.username=your_cassandra_username
spark.cdm.connect.origin.password=your_cassandra_password
spark.cdm.connect.origin.keyspace=your_keyspace

# =============================================================================
# YUGABYTEDB CONNECTION SETTINGS
# =============================================================================
spark.cdm.connect.target.yugabyte.host=vcausc11udev057.azr.bank-dns.com
spark.cdm.connect.target.yugabyte.port=5433
spark.cdm.connect.target.yugabyte.database=cdmybtest
spark.cdm.connect.target.yugabyte.username=yugabyte
spark.cdm.connect.target.yugabyte.password=your_password

# =============================================================================
# CONNECTION POOLING & LIMITS (Addresses "too many clients" error)
# =============================================================================
# Limit concurrent connections to prevent overwhelming YugabyteDB
spark.cdm.connect.target.yugabyte.maxConnections=10
spark.cdm.connect.target.yugabyte.connectionTimeout=30000
spark.cdm.connect.target.yugabyte.socketTimeout=60000
spark.cdm.connect.target.yugabyte.loginTimeout=30

# =============================================================================
# SPARK CONFIGURATION
# =============================================================================
# Reduce parallelism to limit concurrent connections
spark.sql.adaptive.enabled=true
spark.sql.adaptive.coalescePartitions.enabled=true
spark.sql.adaptive.coalescePartitions.minPartitionNum=1
spark.sql.adaptive.coalescePartitions.initialPartitionNum=10

# Limit executor cores to reduce connection pressure
spark.executor.cores=2
spark.executor.instances=5
spark.executor.memory=4G
spark.driver.memory=2G

# =============================================================================
# RATE LIMITING (Prevents overwhelming target database)
# =============================================================================
spark.cdm.rate.origin.readsPerSecond=1000
spark.cdm.rate.target.writesPerSecond=500

# =============================================================================
# BATCH PROCESSING
# =============================================================================
spark.cdm.batch.size=100
spark.cdm.fetch.size=1000

# =============================================================================
# LOGGING CONFIGURATION
# =============================================================================
# Custom log directory for failed records
spark.cdm.log.directory=migration_logs

# Reduce log verbosity
spark.driver.extraJavaOptions=-Dlog4j.configuration=file:log4j2-quiet.properties
spark.executor.extraJavaOptions=-Dlog4j.configuration=file:log4j2-quiet.properties

# =============================================================================
# SCHEMA CONFIGURATION
# =============================================================================
spark.cdm.schema.origin.keyspaceTable=customer_datastore.customer_mtrc_by_lpid
spark.cdm.schema.target.keyspaceTable=customer_datastore.customer_mtrc_by_lpid

# =============================================================================
# PERFORMANCE TUNING
# =============================================================================
# Enable compression
spark.cdm.compression.enabled=true

# Optimize for large datasets
spark.cdm.partition.size=1000000
spark.cdm.thread.count=4

# =============================================================================
# ERROR HANDLING
# =============================================================================
# Retry configuration
spark.cdm.retry.maxAttempts=3
spark.cdm.retry.delayMs=2000

# Continue on errors
spark.cdm.continueOnError=true
