# =============================================================================
# CASSANDRA TO YUGABYTE YSQL MIGRATION CONFIGURATION
# =============================================================================
# This configuration migrates data from DataStax Cassandra to YugabyteDB YSQL
# YSQL uses PostgreSQL protocol and requires the YugabyteDB Smart Driver
#
# IMPORTANT: Use this file with the YugabyteMigrate class
# Command: --class com.datastax.cdm.job.YugabyteMigrate
#
# The YugabyteDB Smart Driver provides:
# - HikariCP connection pooling
# - Cluster-aware load balancing
# - Topology-aware routing
# - SSL/TLS support
# =============================================================================

# =============================================================================
# CASSANDRA SOURCE CONFIGURATION
# =============================================================================
spark.cdm.connect.origin.host=vcausc11udev057.azr.bank-dns.com
spark.cdm.connect.origin.port=9042
spark.cdm.connect.origin.username=cassandraappiduat
spark.cdm.connect.origin.password=Uat@123456
spark.cdm.connect.origin.localDC=datacenter1
spark.cdm.connect.origin.consistencyLevel=LOCAL_ONE

# =============================================================================
# YUGABYTE YSQL TARGET CONFIGURATION
# =============================================================================
# YSQL uses PostgreSQL protocol on port 5433 (default YugabyteDB YSQL port)
# These properties use the "yugabyte" prefix to distinguish from YCQL
spark.cdm.connect.target.yugabyte.host=vcausc11udev057.azr.bank-dns.com
spark.cdm.connect.target.yugabyte.port=5433
spark.cdm.connect.target.yugabyte.database=cdmybtest
spark.cdm.connect.target.yugabyte.username=yugabyte
spark.cdm.connect.target.yugabyte.password=password

# =============================================================================
# SCHEMA CONFIGURATION
# =============================================================================
spark.cdm.schema.origin.keyspaceTable=customer_datastore.customer_mtrc_by_lpid
spark.cdm.schema.target.keyspaceTable=customer_datastore.customer_mtrc_by_lpid

# =============================================================================
# SPARK CONFIGURATION
# =============================================================================
# 
# MEMORY CALCULATION FOR CURRENT CONFIGURATION:
# ---------------------------------------------
# Current Settings:
#   - Executor Instances: 4
#   - Executor Memory: 6GB each
#   - Driver Memory: 6GB
#   - Executor Cores: 4 each
#
# Total Executors: 4 instances
# Total Parallelism: 4 instances × 4 cores = 16 parallel tasks
#
# MEMORY REQUIREMENTS:
# --------------------
# Spark Memory Usage:
#   - Executor Memory: 4 × 6GB = 24GB
#   - Driver Memory: 6GB
#   - Total Spark Memory: 24GB + 6GB = 30GB
#
# Additional Memory Needed:
#   - JVM Overhead: ~10-20% of heap (3-6GB)
#   - OS & System: 2-4GB
#   - Network Buffers: 1-2GB
#   - Other Processes: 2-4GB
#
# MINIMUM MACHINE MEMORY REQUIRED: ~40-48GB
# RECOMMENDED MACHINE MEMORY: 48-64GB (for safety margin)
#
# IMPORTANT NOTES:
# ----------------
# 1. LOCAL MODE (--master "local[*]"):
#    - In local mode, executors run in the same JVM as driver
#    - Actual memory usage may be less than calculated above
#    - Spark may ignore executor.instances or use it differently
#    - Total memory ≈ driver.memory + executor.memory (not multiplied)
#    - For local mode: ~12-16GB minimum, 24GB recommended
#
# 2. CLUSTER MODE (--master "spark://master:port"):
#    - Each executor runs in separate JVM on different nodes
#    - Full calculation applies: 4 × 6GB + 6GB = 30GB+ overhead
#    - Each worker node needs: 6GB + overhead (~8-10GB per node)
#    - Driver node needs: 6GB + overhead (~10-12GB)
#
# 3. ADJUSTING MEMORY:
#    - If machine has less memory, reduce executor.instances or executor.memory
#    - Example: 2 instances × 4GB = 8GB + 4GB driver = 12GB (needs ~20GB machine)
#    - Example: 1 instance × 8GB = 8GB + 6GB driver = 14GB (needs ~24GB machine)
#
# Resource Allocation Parameters:
# ------------------------------
# Number of CPU cores per executor (worker node)
# - Controls parallelism within each executor
# - Higher values = more parallel tasks per executor
# - Recommended: 2-8 cores per executor (depends on workload)
spark.executor.cores=4

# Number of executor instances (worker processes)
# - Total executors = instances × cores = 4 × 4 = 16 parallel tasks
# - For local mode: typically 1-4 instances
# - For cluster mode: adjust based on cluster size
# - CURRENT: 4 executors = 4 × 6GB = 24GB executor memory
spark.executor.instances=4

# Memory allocated to each executor (heap size)
# - Used for data caching, shuffles, and task execution
# - Leave ~10-20% for overhead (OS, JVM, etc.)
# - Example: 6G executor memory = ~5G usable for Spark
# - CURRENT: 4 executors × 6GB = 24GB total executor memory
spark.executor.memory=6G

# Memory allocated to the Spark driver (master/coordinator)
# - Driver coordinates tasks, collects results, manages metadata
# - Needs more memory if collecting large results or broadcasting large tables
# - For local mode: should match or exceed executor memory
# - CURRENT: 6GB driver memory
spark.driver.memory=6G

# Adaptive Query Execution (AQE) Parameters:
# -----------------------------------------
# Enable Adaptive Query Execution (Spark 3.0+)
# - Dynamically optimizes query execution based on runtime statistics
# - Automatically adjusts join strategies, partition sizes, etc.
# - Recommended: Always enable for better performance
spark.sql.adaptive.enabled=true

# Enable adaptive partition coalescing
# - Combines small partitions after shuffle operations
# - Reduces overhead from too many small partitions
# - Improves performance when partitions are unevenly sized
spark.sql.adaptive.coalescePartitions.enabled=true

# Enable adaptive skew join handling
# - Detects and handles data skew in joins automatically
# - Splits large partitions to balance workload
# - Critical for joins with uneven data distribution
spark.sql.adaptive.skewJoin.enabled=true

# Serialization Parameters:
# -------------------------
# Use Kryo serializer instead of default Java serializer
# - Kryo is faster and more efficient (2-10x faster)
# - Produces smaller serialized data (better network transfer)
# - Recommended: Always use Kryo for better performance
spark.serializer=org.apache.spark.serializer.KryoSerializer

# Adaptive Execution Tuning:
# ---------------------------
# Target partition size for adaptive execution
# - AQE tries to create partitions of this size
# - Smaller = more partitions = more parallelism but more overhead
# - Larger = fewer partitions = less overhead but less parallelism
# - Recommended: 32-128MB depending on data size
spark.sql.adaptive.advisoryPartitionSizeInBytes=32MB

# Timeout Parameters:
# -------------------
# Network timeout for all network operations
# - Time to wait for network I/O (fetching data, shuffles, etc.)
# - Increase if you have slow network or large data transfers
# - Default: 120s, but 600s (10 min) is safer for large migrations
spark.network.timeout=600s

# Broadcast join timeout
# - Time to wait for broadcast tables to be distributed to executors
# - Increase if broadcasting large tables (>100MB)
# - Should be >= network.timeout for large broadcasts
spark.sql.broadcastTimeout=600s

# =============================================================================
# CDM PERFORMANCE CONFIGURATION
# =============================================================================
#
# THROUGHPUT CALCULATION FOR CURRENT CONFIGURATION:
# --------------------------------------------------
# Current Settings:
#   - numParts: 400 partitions
#   - Rate Limit (Origin): 5000 reads/second
#   - Rate Limit (Target): 5000 writes/second
#   - Batch Size: 5000 records per batch
#   - Fetch Size: 5000 rows per fetch
#   - Executors: 4 instances × 4 cores = 16 parallel tasks
#
# THROUGHPUT ANALYSIS:
# --------------------
# 1. LOCAL MODE (--master "local[*]"):
#    - Rate limits apply to ENTIRE job (not per executor)
#    - Maximum Throughput: MIN(5000 reads/sec, 5000 writes/sec) = 5000 records/second
#    - Effective Throughput: ~4000-4500 records/second (accounting for overhead)
#    
#    CALCULATION BREAKDOWN:
#    - Base rate limit: 5000 records/second
#    - Accounting for overhead (network, processing, etc.): ~10-20% reduction
#    - Effective: 5000 × 0.8 to 5000 × 0.9 = 4000-4500 records/second
#    
#    RECORDS PER HOUR:
#    - 4000 records/sec × 3600 seconds = 14,400,000 records = ~14.4 million/hour
#    - 4500 records/sec × 3600 seconds = 16,200,000 records = ~16.2 million/hour
#    - Range: ~14-16 million records/hour
#    
#    RECORDS PER DAY:
#    - 4000 records/sec × 86400 seconds = 345,600,000 records = ~345.6 million/day
#    - 4500 records/sec × 86400 seconds = 388,800,000 records = ~388.8 million/day
#    - Range: ~345-380 million records/day
#
# 2. CLUSTER MODE (--master "spark://master:port"):
#    - Rate limits apply PER EXECUTOR (per worker node)
#    - With 4 executors: 4 × 5000 = 20,000 records/second theoretical max
#    - Effective Throughput: ~16,000-18,000 records/second (accounting for overhead)
#    
#    CALCULATION BREAKDOWN:
#    - Per executor: 5000 records/second
#    - Total with 4 executors: 4 × 5000 = 20,000 records/second
#    - Accounting for overhead: ~10-20% reduction
#    - Effective: 20000 × 0.8 to 20000 × 0.9 = 16,000-18,000 records/second
#    
#    RECORDS PER HOUR:
#    - 16000 records/sec × 3600 seconds = 57,600,000 records = ~57.6 million/hour
#    - 18000 records/sec × 3600 seconds = 64,800,000 records = ~64.8 million/hour
#    - Range: ~57-65 million records/hour
#    
#    RECORDS PER DAY:
#    - 16000 records/sec × 86400 seconds = 1,382,400,000 records = ~1.38 billion/day
#    - 18000 records/sec × 86400 seconds = 1,555,200,000 records = ~1.56 billion/day
#    - Range: ~1.4-1.6 billion records/day
#
# FORMULA REFERENCE:
# ------------------
# Records per hour = (records/second) × 3600
# Records per day = (records/second) × 86400
#
# If you increase rate limits, throughput increases proportionally:
# Example: If rate limit = 10000 (doubled from 5000):
#   - Local mode: 8000-9000 records/sec = ~28-32 million/hour = ~690-780 million/day
#   - Cluster mode (4 executors): 32000-36000 records/sec = ~115-130 million/hour = ~2.8-3.1 billion/day
#
# BOTTLENECK ANALYSIS:
# --------------------
# The throughput is LIMITED by the SMALLER of:
#   - Origin read rate limit: 5000 reads/sec
#   - Target write rate limit: 5000 writes/sec
#   - Network bandwidth
#   - Database cluster capacity
#
# To increase throughput:
#   - Increase rate limits (if your clusters can handle it)
#   - Increase numParts (more parallelism)
#   - Use cluster mode instead of local mode
#   - Optimize batchSize and fetchSize based on row size
#
# Parameter Explanations:
# ------------------------
# Number of partitions to divide the token range
# - The full token range (-2^63 to 2^63-1) is divided into this many parts
# - Each partition is processed in parallel
# - Ideal value: table-size / 10MB (aim for ~10MB per partition)
# - Example: 4GB table = 400 partitions (4GB / 10MB = 400)
# - Too few partitions = less parallelism, slower migration
# - Too many partitions = more overhead, may not improve performance
# - CURRENT: 400 partitions = good for ~4GB tables
# - For larger tables, increase proportionally (e.g., 40GB table = 4000 partitions)
spark.cdm.perfops.numParts=400

# Maximum read operations per second from Origin (Cassandra)
# - Rate limiter controls how fast CDM reads from source cluster
# - Prevents overwhelming the origin cluster
# - Applied per executor in cluster mode, globally in local mode
# - Default: 20000, but should be tuned based on origin cluster capacity
# - CURRENT: 5000 reads/second (conservative, safe for most clusters)
# - To increase: Monitor origin cluster load, gradually increase if headroom available
# - Formula: Effective reads/sec = ratelimit.origin × executor.instances (cluster mode)
spark.cdm.perfops.ratelimit.origin=5000

# Maximum write operations per second to Target (YugabyteDB)
# - Rate limiter controls how fast CDM writes to target cluster
# - Prevents overwhelming the target cluster
# - Applied per executor in cluster mode, globally in local mode
# - Default: 20000, but should be tuned based on target cluster capacity
# - CURRENT: 5000 writes/second (conservative, safe for most clusters)
# - To increase: Monitor target cluster load, gradually increase if headroom available
# - Formula: Effective writes/sec = ratelimit.target × executor.instances (cluster mode)
# - IMPORTANT: Throughput = MIN(ratelimit.origin, ratelimit.target)
spark.cdm.perfops.ratelimit.target=5000

# Number of records to batch together when writing to Target
# - Records are grouped into UNLOGGED batches for efficient writes
# - Larger batches = fewer network round-trips = better throughput
# - But: Too large = memory pressure, timeout risks
# - Default: 5 records per batch
# - CURRENT: 5000 records per batch (very large, good for small rows)
# - Tuning Guidelines:
#   * Small rows (< 1KB): Use 100-5000 (current setting is good)
#   * Medium rows (1-10KB): Use 100-1000
#   * Large rows (10-100KB): Use 10-100
#   * Very large rows (> 100KB): Use 1-10
#   * If primary-key = partition-key: Use 1 (no batching benefit)
# - Memory impact: batchSize × avg_row_size = memory per batch
#   Example: 5000 × 1KB = 5MB per batch (manageable)
spark.cdm.perfops.batchSize=5000

# Number of rows to fetch from Origin in each read operation
# - Controls how many rows are read from Cassandra in one query
# - Larger fetch = fewer queries = better throughput
# - But: Too large = more memory usage, longer query times
# - Default: 1000 rows per fetch
# - CURRENT: 5000 rows per fetch (good for small-medium rows)
# - Tuning Guidelines:
#   * Small rows (< 1KB): Use 1000-10000 (current setting is good)
#   * Medium rows (1-10KB): Use 500-5000
#   * Large rows (10-100KB): Use 100-1000
#   * Very large rows (> 100KB): Use 10-100
# - Memory impact: fetchSize × avg_row_size = memory per fetch
#   Example: 5000 × 1KB = 5MB per fetch (manageable)
# - Relationship with batchSize: fetchSize should be >= batchSize for efficiency
spark.cdm.perfops.fetchSizeInRows=5000

# =============================================================================
# CONNECTION SETTINGS
# =============================================================================
spark.cdm.connect.origin.timeout=120000
spark.cdm.connect.target.yugabyte.timeout=120000

# =============================================================================
# HIKARICP CONNECTION POOL CONFIGURATION (YugabyteDB Smart Driver)
# =============================================================================
# HikariCP pool settings for YBClusterAwareDataSource
# Reference: https://docs.yugabyte.com/preview/develop/drivers-orms/java/yugabyte-jdbc-reference/
spark.cdm.connect.target.yugabyte.pool.maxSize=10
spark.cdm.connect.target.yugabyte.pool.minSize=2

# Optional: Additional endpoints for load balancing (comma-separated host:port)
# Example: spark.cdm.connect.target.yugabyte.additionalEndpoints=127.0.0.2:5433,127.0.0.3:5433
# spark.cdm.connect.target.yugabyte.additionalEndpoints=

# Optional: Topology keys for geo-location aware load balancing
# Format: cloud.region.zone (e.g., aws.us-east-1.us-east-1a)
# Example: spark.cdm.connect.target.yugabyte.topologyKeys=aws.us-east-1.us-east-1a
# spark.cdm.connect.target.yugabyte.topologyKeys=

# =============================================================================
# SSL/TLS CONFIGURATION (Optional)
# =============================================================================
# SSL is disabled by default. Uncomment and configure if SSL is required
# spark.cdm.connect.target.yugabyte.ssl.enabled=true
# spark.cdm.connect.target.yugabyte.sslmode=require
# spark.cdm.connect.target.yugabyte.sslrootcert=/path/to/ca.crt

# =============================================================================
# AUTOCORRECT AND TRACKING
# =============================================================================
spark.cdm.autocorrect.missing=false
spark.cdm.autocorrect.mismatch=false
spark.cdm.trackRun=false

# =============================================================================
# LOGGING
# =============================================================================
spark.cdm.log.directory=migration_logs
spark.cdm.log.level=INFO

