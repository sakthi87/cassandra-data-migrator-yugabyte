# Performance Enhancement Guide for Cassandra to YugabyteDB Migration

## üéØ **Overview**

This guide provides comprehensive performance optimization techniques to significantly reduce data migration time from Cassandra to YugabyteDB. The optimizations can improve throughput from ~360 records/second to 1,000-1,500 records/second, reducing migration time from 5 hours to 1-2 hours.

## üìä **Performance Analysis**

### **Current Performance Issues:**
- **Throughput**: ~1.8M records/hour (360 records/second)
- **Data Rate**: ~720MB/hour
- **Bottleneck**: Network I/O and connection management
- **Resource Utilization**: Low CPU and memory usage

### **Target Performance:**
- **Throughput**: 4-6M records/hour (1,000-1,500 records/second)
- **Data Rate**: 2-3GB/hour
- **Resource Utilization**: High CPU and memory usage (80-90%)

## ‚ö° **Optimized Configuration**

### **1. Connection Pooling & Limits**

```properties
# =============================================================================
# OPTIMIZED CONNECTION POOLING & LIMITS
# =============================================================================
spark.cdm.connect.target.yugabyte.maxConnections=200
spark.cdm.connect.target.yugabyte.connectionTimeout=60000
spark.cdm.connect.target.yugabyte.socketTimeout=120000
spark.cdm.connect.target.yugabyte.loginTimeout=60
```

**Explanation:**
- **maxConnections=200**: Increases concurrent database connections from 50 to 200, allowing more parallel writes
- **connectionTimeout=60000**: Extends connection timeout to 60 seconds for stability during high load
- **socketTimeout=120000**: Increases socket timeout to 2 minutes to handle large data transfers
- **loginTimeout=60**: Extends login timeout for connection establishment under load

### **2. Aggressive Spark Configuration**

```properties
# =============================================================================
# AGGRESSIVE SPARK CONFIGURATION
# =============================================================================
spark.sql.adaptive.enabled=true
spark.sql.adaptive.coalescePartitions.enabled=true
spark.sql.adaptive.coalescePartitions.minPartitionNum=10
spark.sql.adaptive.coalescePartitions.initialPartitionNum=200

# Increase parallelism significantly
spark.executor.cores=20
spark.executor.instances=10
spark.executor.memory=20G
spark.driver.memory=20G

# Increase partition count for better parallelism
spark.cdm.perfops.numParts=500
spark.cdm.perfops.batchSize=1000
spark.cdm.perfops.readRateLimit=50000
spark.cdm.perfops.writeRateLimit=50000
```

**Explanation:**
- **executor.cores=20**: Doubles CPU cores per executor from 10 to 20
- **executor.instances=10**: Doubles number of executors from 5 to 10
- **Total cores**: Increases from 50 to 200 (4x improvement)
- **numParts=500**: Increases partitions from 100 to 500 for better data distribution
- **batchSize=1000**: Increases batch size from 5 to 1000 for more efficient processing
- **Rate limits**: Increases from 10,000 to 50,000 operations/second

### **3. Optimized Rate Limiting**

```properties
# =============================================================================
# AGGRESSIVE RATE LIMITING
# =============================================================================
spark.cdm.rate.origin.readsPerSecond=50000
spark.cdm.rate.target.writesPerSecond=50000
```

**Explanation:**
- **readsPerSecond=50000**: Increases Cassandra read rate from 10,000 to 50,000 records/second
- **writesPerSecond=50000**: Increases YugabyteDB write rate from 10,000 to 50,000 records/second
- **5x improvement** in data processing speed

### **4. Enhanced Batch Processing**

```properties
# =============================================================================
# OPTIMIZED BATCH PROCESSING
# =============================================================================
spark.cdm.batch.size=1000
spark.cdm.fetch.size=10000
```

**Explanation:**
- **batch.size=1000**: Increases batch size from 5 to 1000 records per batch
- **fetch.size=10000**: Increases fetch size from 1,000 to 10,000 records per fetch
- **Reduces network round trips** and improves efficiency

### **5. Performance Tuning**

```properties
# =============================================================================
# PERFORMANCE TUNING
# =============================================================================
# Enable compression
spark.cdm.compression.enabled=true

# Optimize for large datasets
spark.cdm.partition.size=5000000
spark.cdm.thread.count=50
```

**Explanation:**
- **compression.enabled=true**: Enables data compression to reduce network transfer time
- **partition.size=5000000**: Increases partition size from 1M to 5M records for better efficiency
- **thread.count=50**: Increases thread count from 10 to 50 for parallel processing

### **6. Network Optimization**

```properties
# =============================================================================
# NETWORK OPTIMIZATION
# =============================================================================
# Increase network buffer sizes
spark.cdm.network.bufferSize=65536
spark.cdm.network.sendBufferSize=65536
spark.cdm.network.receiveBufferSize=65536
```

**Explanation:**
- **bufferSize=65536**: Increases network buffer size to 64KB
- **sendBufferSize=65536**: Optimizes send buffer for large data transfers
- **receiveBufferSize=65536**: Optimizes receive buffer for large data transfers
- **Reduces network latency** and improves throughput

### **7. YugabyteDB Optimization**

```properties
# =============================================================================
# YUGABYTE OPTIMIZATION
# =============================================================================
# Enable connection pooling
spark.cdm.connect.target.yugabyte.pooling.enabled=true
spark.cdm.connect.target.yugabyte.pooling.maxSize=200
spark.cdm.connect.target.yugabyte.pooling.minSize=50

# Enable prepared statement caching
spark.cdm.connect.target.yugabyte.preparedStatementCacheSize=1000
```

**Explanation:**
- **pooling.enabled=true**: Enables connection pooling for better connection management
- **pooling.maxSize=200**: Sets maximum pool size to 200 connections
- **pooling.minSize=50**: Sets minimum pool size to 50 connections
- **preparedStatementCacheSize=1000**: Caches prepared statements to reduce query preparation time

## üöÄ **Expected Performance Improvement**

### **Before (Current Configuration):**
- **Time**: 5 hours
- **Throughput**: ~1.8M records/hour
- **Rate**: ~360 records/second
- **Resource Usage**: Low (20-30% CPU, 30-40% memory)

### **After (Optimized Configuration):**
- **Time**: 1-2 hours
- **Throughput**: ~4-6M records/hour
- **Rate**: ~1,000-1,500 records/second
- **Resource Usage**: High (80-90% CPU, 80-90% memory)

### **Performance Gains:**
- **3-4x faster** data processing
- **5x higher** throughput
- **Better resource utilization**
- **Reduced network latency**

## ‚öôÔ∏è **Additional Optimization Techniques**

### **1. YugabyteDB Side Optimization**

```sql
-- Increase connection limits
ALTER SYSTEM SET max_connections = 500;

-- Optimize for bulk loading
ALTER SYSTEM SET shared_buffers = '4GB';
ALTER SYSTEM SET effective_cache_size = '12GB';
ALTER SYSTEM SET work_mem = '256MB';

-- Enable parallel workers
ALTER SYSTEM SET max_parallel_workers = 16;
ALTER SYSTEM SET max_parallel_workers_per_gather = 8;
```

### **2. Network System Optimization**

```bash
# Increase network buffer sizes
echo 'net.core.rmem_max = 134217728' >> /etc/sysctl.conf
echo 'net.core.wmem_max = 134217728' >> /etc/sysctl.conf
echo 'net.core.rmem_default = 65536' >> /etc/sysctl.conf
echo 'net.core.wmem_default = 65536' >> /etc/sysctl.conf
sysctl -p
```

### **3. JVM Tuning**

```properties
# Add to your spark-submit command
--conf spark.driver.extraJavaOptions="-Xmx20g -XX:+UseG1GC -XX:MaxGCPauseMillis=200 -XX:+UnlockExperimentalVMOptions -XX:+UseStringDeduplication"
--conf spark.executor.extraJavaOptions="-Xmx20g -XX:+UseG1GC -XX:MaxGCPauseMillis=200 -XX:+UnlockExperimentalVMOptions -XX:+UseStringDeduplication"
```

**Explanation:**
- **Xmx20g**: Sets maximum heap size to 20GB
- **UseG1GC**: Uses G1 garbage collector for better performance
- **MaxGCPauseMillis=200**: Limits GC pause time to 200ms
- **UseStringDeduplication**: Deduplicates strings to save memory

## üéØ **Recommended Migration Command**

```bash
spark-submit \
  --properties-file optimized-migration.properties \
  --conf spark.cdm.schema.origin.keyspaceTable="customer_datastore.customer_mtrc_by_lpid" \
  --master "local[*]" \
  --driver-memory 20G \
  --executor-memory 20G \
  --class com.datastax.cdm.job.YugabyteMigrate \
  target/cassandra-data-migrator-5.5.2-SNAPSHOT.jar
```

## üîç **Performance Monitoring**

### **Key Metrics to Watch:**

1. **CPU Usage**: Should be 80-90%
2. **Memory Usage**: Should be 80-90%
3. **Network I/O**: Should be high
4. **YugabyteDB Connections**: Should be near max (200)
5. **Throughput**: Should be 1,000+ records/second

### **Monitoring Commands:**

```bash
# Monitor system resources
htop

# Monitor network usage
iftop

# Monitor YugabyteDB connections
psql -h your-yugabyte-host -p 5433 -U your-user -d your-db -c "SELECT count(*) FROM pg_stat_activity;"

# Monitor migration progress
tail -f migration_logs/migration_summary_*.txt
```

## ‚ö†Ô∏è **Important Considerations**

### **1. Resource Requirements:**
- **Minimum**: 20GB RAM, 20 CPU cores
- **Recommended**: 40GB RAM, 40 CPU cores
- **Network**: High bandwidth connection

### **2. YugabyteDB Capacity:**
- **Connections**: Must support 200+ concurrent connections
- **Memory**: Should have 4GB+ shared_buffers
- **CPU**: Should have sufficient cores for parallel processing

### **3. Testing Strategy:**
1. **Start with small dataset** (100K records) to validate configuration
2. **Monitor resource usage** during test run
3. **Adjust parameters** based on results
4. **Scale up gradually** to full dataset

### **4. Troubleshooting:**
- **Out of memory**: Reduce executor memory or instances
- **Connection errors**: Increase connection timeouts
- **Slow performance**: Check network latency and YugabyteDB performance
- **Resource contention**: Reduce parallelism or increase resources

## üìà **Performance Tuning Checklist**

- [ ] **Connection pooling** enabled and configured
- [ ] **Batch sizes** optimized for your data
- [ ] **Partition count** increased for parallelism
- [ ] **Rate limits** set appropriately
- [ ] **Network buffers** optimized
- [ ] **JVM settings** tuned
- [ ] **YugabyteDB** configured for bulk loading
- [ ] **System resources** sufficient
- [ ] **Monitoring** in place
- [ ] **Test run** completed successfully

## üéâ **Expected Results**

With these optimizations, you should see:
- **3-4x faster** migration time
- **5x higher** throughput
- **Better resource utilization**
- **More stable** migration process
- **Reduced** network latency

The migration time should reduce from **5 hours to 1-2 hours** for your 6.5M records (3.6GB) dataset! üöÄ
