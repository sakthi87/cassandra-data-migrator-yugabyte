# =============================================================================
# HIGH PERFORMANCE MIGRATION CONFIGURATION
# =============================================================================
# This configuration is optimized for maximum throughput
# Expected: 3-4x performance improvement (5.5 hours â†’ 1.5-2 hours)

# =============================================================================
# CONNECTION POOLING & LIMITS
# =============================================================================
spark.cdm.connect.target.yugabyte.maxConnections=500
spark.cdm.connect.target.yugabyte.connectionTimeout=120000
spark.cdm.connect.target.yugabyte.socketTimeout=300000
spark.cdm.connect.target.yugabyte.loginTimeout=120

# =============================================================================
# AGGRESSIVE SPARK CONFIGURATION
# =============================================================================
spark.sql.adaptive.enabled=true
spark.sql.adaptive.coalescePartitions.enabled=false
spark.sql.adaptive.coalescePartitions.minPartitionNum=1
spark.sql.adaptive.coalescePartitions.initialPartitionNum=1000

# Maximum parallelism
spark.executor.cores=25
spark.executor.instances=15
spark.executor.memory=25G
spark.driver.memory=25G

# High partition count for maximum parallelism
spark.cdm.perfops.numParts=1000
spark.cdm.perfops.batchSize=5000
spark.cdm.perfops.readRateLimit=100000
spark.cdm.perfops.writeRateLimit=100000

# =============================================================================
# AGGRESSIVE RATE LIMITING
# =============================================================================
spark.cdm.rate.origin.readsPerSecond=100000
spark.cdm.rate.target.writesPerSecond=100000

# =============================================================================
# OPTIMIZED BATCH PROCESSING
# =============================================================================
spark.cdm.batch.size=5000
spark.cdm.fetch.size=50000

# =============================================================================
# NETWORK OPTIMIZATION
# =============================================================================
spark.cdm.network.bufferSize=131072
spark.cdm.network.sendBufferSize=131072
spark.cdm.network.receiveBufferSize=131072

# =============================================================================
# PERFORMANCE TUNING
# =============================================================================
# Enable compression
spark.cdm.compression.enabled=true

# Optimize for large datasets
spark.cdm.partition.size=10000000
spark.cdm.thread.count=100

# =============================================================================
# YUGABYTE OPTIMIZATION
# =============================================================================
# Enable connection pooling
spark.cdm.connect.target.yugabyte.pooling.enabled=true
spark.cdm.connect.target.yugabyte.pooling.maxSize=500
spark.cdm.connect.target.yugabyte.pooling.minSize=100

# Enable prepared statement caching
spark.cdm.connect.target.yugabyte.preparedStatementCacheSize=2000

# =============================================================================
# ERROR HANDLING
# =============================================================================
spark.cdm.retry.maxAttempts=3
spark.cdm.retry.delayMs=1000
spark.cdm.continueOnError=true

# =============================================================================
# LOGGING CONFIGURATION
# =============================================================================
spark.cdm.log.directory=migration_logs
spark.driver.extraJavaOptions=-Dlog4j.configuration=file:log4j2-quiet.properties
spark.executor.extraJavaOptions=-Dlog4j.configuration=file:log4j2-quiet.properties

# =============================================================================
# ADDITIONAL PERFORMANCE PARAMETERS
# =============================================================================
# Disable adaptive query execution for consistent performance
spark.sql.adaptive.enabled=false

# Increase shuffle partitions
spark.sql.shuffle.partitions=1000

# Optimize serialization
spark.serializer=org.apache.spark.serializer.KryoSerializer
spark.kryo.unsafe=true
spark.kryo.registrationRequired=false

# Increase network timeout
spark.network.timeout=800s
spark.sql.broadcastTimeout=600s

# Optimize memory management
spark.sql.adaptive.skewJoin.enabled=false
spark.sql.adaptive.localShuffleReader.enabled=false
