# =============================================================================
# CDM Migration Properties - Transaction Table Test WITH AUDIT FIELDS
# =============================================================================
# This file is configured for testing migration from Cassandra to YugabyteDB
# with Constant Columns feature to populate audit fields
# Source: transaction_datastore.dda_pstd_fincl_txn_cnsmr_by_accntnbr
# Target: transaction_datastore.dda_pstd_fincl_txn_cnsmr_by_accntnbr
# =============================================================================

# =============================================================================
# ORIGIN (CASSANDRA) CONNECTION
# =============================================================================
spark.cdm.connect.origin.host=localhost
spark.cdm.connect.origin.port=9043
spark.cdm.connect.origin.username=cassandra
spark.cdm.connect.origin.password=cassandra

# =============================================================================
# TARGET (YUGABYTEDB YSQL) CONNECTION
# =============================================================================
spark.cdm.connect.target.yugabyte.host=localhost
spark.cdm.connect.target.yugabyte.port=5433
spark.cdm.connect.target.yugabyte.database=transaction_datastore
spark.cdm.connect.target.yugabyte.username=yugabyte
spark.cdm.connect.target.yugabyte.password=yugabyte
# Schema name (optional - defaults to "public" if not specified)
spark.cdm.connect.target.yugabyte.schema=public

# =============================================================================
# SCHEMA CONFIGURATION
# =============================================================================
# Origin (Cassandra) - needs keyspace.table format
spark.cdm.schema.origin.keyspaceTable=transaction_datastore.dda_pstd_fincl_txn_cnsmr_by_accntnbr
# Target (YugabyteDB) - table name only (database and schema are defined in connection parameters above)
spark.cdm.schema.target.keyspaceTable=dda_pstd_fincl_txn_cnsmr_by_accntnbr

# =============================================================================
# CONSTANT COLUMNS FEATURE - AUDIT FIELDS
# =============================================================================
# Populate audit fields that don't exist in source table
# These fields will be populated with constant values for all migrated records
spark.cdm.feature.constantColumns.names=z_audit_crtd_by_txt,z_audit_evnt_id,z_audit_crtd_ts,z_audit_last_mdfd_by_txt
spark.cdm.feature.constantColumns.values='CDM_MIGRATION','MIGRATION_BATCH_001','2024-12-17T10:00:00Z','CDM_MIGRATION'

# =============================================================================
# HIGH-PERFORMANCE SETTINGS (Phase 1+2 Optimizations)
# =============================================================================
# Phase 1: PreparedStatement reuse
# Phase 2: JDBC batching with rewriteBatchedInserts
# Batch size - optimized for 3K+ IOPS
# Using 50 for optimal balance (40-partition config achieved 3,159 IOPS)
spark.cdm.connect.target.yugabyte.batchSize=50
spark.cdm.connect.target.yugabyte.rewriteBatchedInserts=true
# Load Balancing Configuration
# For 3-node Azure Central cluster: Enable with topology keys for optimal performance
# For multi-region: REQUIRED - enables geo-location aware load balancing
# 
# To enable, uncomment and set your topology keys:
# spark.cdm.connect.target.yugabyte.loadBalance=true
# spark.cdm.connect.target.yugabyte.topologyKeys=azure.centralus.zone1,azure.centralus.zone2,azure.centralus.zone3
#
# For now, keep disabled if topology keys are unknown
spark.cdm.connect.target.yugabyte.loadBalance=false
spark.cdm.connect.target.yugabyte.prepareThreshold=5
spark.cdm.connect.target.yugabyte.socketTimeout=60000
spark.cdm.connect.target.yugabyte.tcpKeepAlive=true

# Connection Pool Settings
# CRITICAL: With numParts=40, total connections = 40 Ã— maxSize
# Optimized for 3K+ IOPS: maxSize = 3 (120 total connections) - proven configuration
spark.cdm.connect.target.yugabyte.pool.maxSize=3
spark.cdm.connect.target.yugabyte.pool.minSize=1

# =============================================================================
# PERFORMANCE SETTINGS - OPTIMIZED FOR 3K+ IOPS
# =============================================================================
# CRITICAL: Match numParts with Spark parallelism
# Best configuration: 40 partitions achieved 3,159 IOPS average, 3,930 IOPS peak
# Spark parallelism should match partitions (20 for local[20])
spark.cdm.perfops.numParts=40
spark.cdm.perfops.batchSize=25
# Rate limits - set high or comment out to disable
# For maximum throughput, comment out these lines
# spark.cdm.perfops.ratelimit.origin=50000
# spark.cdm.perfops.ratelimit.target=50000
spark.cdm.perfops.fetchSizeInRows=2000

# Consistency Levels
spark.cdm.perfops.consistency.read=LOCAL_ONE
spark.cdm.perfops.consistency.write=LOCAL_QUORUM

# =============================================================================
# LOGGING
# =============================================================================
spark.cdm.log.directory=migration_logs
spark.cdm.log.level=INFO

# =============================================================================
# TRACKING (Optional - for resumable migrations)
# =============================================================================
spark.cdm.trackRun=false

