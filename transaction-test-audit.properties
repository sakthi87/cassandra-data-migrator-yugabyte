# =============================================================================
# CDM Migration Properties - Transaction Table Test WITH AUDIT FIELDS
# =============================================================================
# This file is configured for testing migration from Cassandra to YugabyteDB
# with Constant Columns feature to populate audit fields
# Source: transaction_datastore.dda_pstd_fincl_txn_cnsmr_by_accntnbr
# Target: transaction_datastore.dda_pstd_fincl_txn_cnsmr_by_accntnbr
# =============================================================================

# =============================================================================
# ORIGIN (CASSANDRA) CONNECTION
# =============================================================================
spark.cdm.connect.origin.host=localhost
spark.cdm.connect.origin.port=9043
spark.cdm.connect.origin.username=cassandra
spark.cdm.connect.origin.password=cassandra

# =============================================================================
# TARGET (YUGABYTEDB YSQL) CONNECTION
# =============================================================================
spark.cdm.connect.target.yugabyte.host=localhost
spark.cdm.connect.target.yugabyte.port=5433
spark.cdm.connect.target.yugabyte.database=transaction_datastore
spark.cdm.connect.target.yugabyte.username=yugabyte
spark.cdm.connect.target.yugabyte.password=yugabyte
# Schema name (optional - defaults to "public" if not specified)
spark.cdm.connect.target.yugabyte.schema=transactions

# =============================================================================
# SCHEMA CONFIGURATION
# =============================================================================
# Origin (Cassandra) - needs keyspace.table format
spark.cdm.schema.origin.keyspaceTable=transaction_datastore.dda_pstd_fincl_txn_cnsmr_by_accntnbr
# Target (YugabyteDB) - table name only (database and schema are defined in connection parameters above)
spark.cdm.schema.target.keyspaceTable=dda_pstd_fincl_txn_cnsmr_by_accntnbr

# =============================================================================
# CONSTANT COLUMNS FEATURE - AUDIT FIELDS
# =============================================================================
# Populate audit fields that don't exist in source table
# These fields will be populated with constant values for all migrated records
spark.cdm.feature.constantColumns.names=z_audit_crtd_by_txt,z_audit_evnt_id,z_audit_crtd_ts,z_audit_last_mdfd_by_txt
spark.cdm.feature.constantColumns.values='CDM_MIGRATION','MIGRATION_BATCH_001','2024-12-17T10:00:00Z','CDM_MIGRATION'

# =============================================================================
# HIGH-PERFORMANCE SETTINGS (Phase 1+2 Optimizations)
# =============================================================================
# Phase 1: PreparedStatement reuse
# Phase 2: JDBC batching with rewriteBatchedInserts
# Batch size - increase for better throughput (especially with network latency)
# For cross-datacenter (on-prem to Azure), use 50-100
spark.cdm.connect.target.yugabyte.batchSize=50
spark.cdm.connect.target.yugabyte.rewriteBatchedInserts=true
# Load Balancing Configuration
# For 3-node Azure Central cluster: Enable with topology keys for optimal performance
# For multi-region: REQUIRED - enables geo-location aware load balancing
# 
# To enable, uncomment and set your topology keys:
# spark.cdm.connect.target.yugabyte.loadBalance=true
# spark.cdm.connect.target.yugabyte.topologyKeys=azure.centralus.zone1,azure.centralus.zone2,azure.centralus.zone3
#
# For now, keep disabled if topology keys are unknown
spark.cdm.connect.target.yugabyte.loadBalance=false
spark.cdm.connect.target.yugabyte.prepareThreshold=5
spark.cdm.connect.target.yugabyte.socketTimeout=60000
spark.cdm.connect.target.yugabyte.tcpKeepAlive=true

# Connection Pool Settings
# CRITICAL: With numParts=40, total connections = 40 × maxSize
# Current: 40 × 5 = 200 connections (may exceed YugabyteDB limits)
# Recommended: 40 × 3 = 120 connections (safer, still performant)
#
# Formula options:
# - Conservative: maxSize = 3 (120 total connections)
# - Aggressive: maxSize = 5 (200 total connections) - only if YugabyteDB max_connections > 200
# - Multi-region: maxSize = 10 (400 total connections) - requires higher YugabyteDB limits
#
# Check YugabyteDB max_connections: SELECT setting FROM pg_settings WHERE name = 'max_connections';
spark.cdm.connect.target.yugabyte.pool.maxSize=3
spark.cdm.connect.target.yugabyte.pool.minSize=1

# =============================================================================
# PERFORMANCE SETTINGS - OPTIMIZED FOR 20K IOPS
# =============================================================================
# CRITICAL: Match numParts with Spark parallelism
# If Spark parallelism = 40, use numParts = 20-40
# For 20K IOPS target, use numParts = 40
spark.cdm.perfops.numParts=40
spark.cdm.perfops.batchSize=25
# Rate limits - set high or comment out to disable
# For maximum throughput, comment out these lines
# spark.cdm.perfops.ratelimit.origin=50000
# spark.cdm.perfops.ratelimit.target=50000
spark.cdm.perfops.fetchSizeInRows=2000

# Consistency Levels
spark.cdm.perfops.consistency.read=LOCAL_ONE
spark.cdm.perfops.consistency.write=LOCAL_QUORUM

# =============================================================================
# LOGGING
# =============================================================================
spark.cdm.log.directory=migration_logs
spark.cdm.log.level=INFO

# =============================================================================
# TRACKING (Optional - for resumable migrations)
# =============================================================================
spark.cdm.trackRun=false

