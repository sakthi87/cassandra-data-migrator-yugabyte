# =============================================================================
# FINAL OPTIMIZED MIGRATION CONFIGURATION
# =============================================================================
# This is the DEFINITIVE configuration for maximum performance
# Based on thorough codebase analysis and your 6.5M records (3.6GB) dataset
# Expected: 3-4x performance improvement (5.5 hours → 1.5-2 hours)

# =============================================================================
# CASSANDRA CONNECTION (Origin Database)
# =============================================================================
# Replace with your actual Cassandra connection details
spark.cdm.connect.origin.host=your_cassandra_host
spark.cdm.connect.origin.port=9042
spark.cdm.connect.origin.username=your_username
spark.cdm.connect.origin.password=your_password
spark.cdm.connect.origin.localDC=datacenter1

# =============================================================================
# YUGABYTEDB CONNECTION (Target Database)
# =============================================================================
# Replace with your actual YugabyteDB connection details
spark.cdm.connect.target.yugabyte.host=your_yugabyte_host
spark.cdm.connect.target.yugabyte.port=5433
spark.cdm.connect.target.yugabyte.database=your_database
spark.cdm.connect.target.yugabyte.username=your_username
spark.cdm.connect.target.yugabyte.password=your_password

# =============================================================================
# SCHEMA CONFIGURATION
# =============================================================================
# Your specific table configuration
spark.cdm.schema.origin.keyspaceTable=customer_datastore.customer_mtrc_by_lpid
spark.cdm.schema.target.keyspaceTable=customer_datastore.customer_mtrc_by_lpid

# =============================================================================
# CONNECTION POOLING & LIMITS (CRITICAL FOR PERFORMANCE)
# =============================================================================
# Maximum concurrent database connections (increased from 50 to 500)
# More connections = more parallel writes = faster migration
spark.cdm.connect.target.yugabyte.maxConnections=500

# Connection timeout in milliseconds (5 minutes for stability)
# Prevents connection drops during high load
spark.cdm.connect.target.yugabyte.connectionTimeout=300000

# Socket timeout in milliseconds (10 minutes for large data transfers)
# Allows time for large batch operations to complete
spark.cdm.connect.target.yugabyte.socketTimeout=600000

# Login timeout in seconds (5 minutes for connection establishment)
# Prevents timeout during initial connection setup
spark.cdm.connect.target.yugabyte.loginTimeout=300

# =============================================================================
# SPARK CONFIGURATION (MAXIMUM PARALLELISM)
# =============================================================================
# Disable adaptive query execution to maintain consistent parallelism
# Adaptive features can reduce parallelism, hurting performance
spark.sql.adaptive.enabled=false
spark.sql.adaptive.coalescePartitions.enabled=false
spark.sql.adaptive.skewJoin.enabled=false
spark.sql.adaptive.localShuffleReader.enabled=false

# Maximum parallelism configuration
# 25 cores per executor × 15 executors = 375 total cores
# This maximizes CPU utilization for parallel processing
spark.executor.cores=25
spark.executor.instances=15
spark.executor.memory=25G
spark.driver.memory=25G

# Shuffle partitions for data distribution
# Higher number = better data distribution across executors
spark.sql.shuffle.partitions=2000

# =============================================================================
# PERFORMANCE OPERATIONS (CRITICAL BATCH CONFIGURATION)
# =============================================================================
# Number of partitions to split data into (increased from 100 to 2000)
# More partitions = better parallelization = faster processing
spark.cdm.perfops.numParts=2000

# BATCH SIZE - THE MOST CRITICAL PARAMETER (increased from 10 to 10000)
# This is the number of records processed together in one database operation
# Your current batchSize=10 means 650,000 database operations
# This batchSize=10000 means only 650 database operations (1000x improvement!)
spark.cdm.perfops.batchSize=10000

# Rate limits for reading from Cassandra (increased from 15,000 to 100,000)
# Higher rate = faster data reading from source
spark.cdm.perfops.readRateLimit=100000

# Rate limits for writing to YugabyteDB (increased from 15,000 to 100,000)
# Higher rate = faster data writing to target
spark.cdm.perfops.writeRateLimit=100000

# =============================================================================
# RATE LIMITING (CONTROLS DATA FLOW SPEED)
# =============================================================================
# Origin read rate limit (records per second from Cassandra)
# Increased from 15,000 to 100,000 for 6.7x faster reading
spark.cdm.rate.origin.readsPerSecond=100000

# Target write rate limit (records per second to YugabyteDB)
# Increased from 15,000 to 100,000 for 6.7x faster writing
spark.cdm.rate.target.writesPerSecond=100000

# =============================================================================
# BATCH PROCESSING (DATA TRANSFER EFFICIENCY)
# =============================================================================
# Batch size for database operations (increased from 10 to 10000)
# This is the single most important parameter for performance
# Larger batches = fewer database round trips = much faster migration
spark.cdm.batch.size=10000

# Fetch size for reading data (increased from 15,000 to 100,000)
# Larger fetch size = fewer read operations = faster data retrieval
spark.cdm.fetch.size=100000

# =============================================================================
# NETWORK OPTIMIZATION (REDUCES LATENCY)
# =============================================================================
# Network buffer sizes in bytes (increased to 128KB)
# Larger buffers = better network throughput = faster data transfer
spark.cdm.network.bufferSize=131072
spark.cdm.network.sendBufferSize=131072
spark.cdm.network.receiveBufferSize=131072

# Network timeouts (increased for stability)
# Prevents timeout during large data transfers
spark.network.timeout=1200s
spark.sql.broadcastTimeout=900s

# =============================================================================
# PERFORMANCE TUNING (SYSTEM OPTIMIZATION)
# =============================================================================
# Enable data compression to reduce network transfer time
# Compressed data transfers faster over network
spark.cdm.compression.enabled=true

# Partition size for data processing (increased to 10M records)
# Larger partitions = more efficient processing = better performance
spark.cdm.partition.size=10000000

# Thread count for parallel processing (increased to 100)
# More threads = more parallel operations = faster processing
spark.cdm.thread.count=100

# =============================================================================
# YUGABYTEDB OPTIMIZATION (TARGET DATABASE TUNING)
# =============================================================================
# Enable connection pooling for better connection management
# Pooling reuses connections instead of creating new ones
spark.cdm.connect.target.yugabyte.pooling.enabled=true

# Maximum pool size (increased to 500 connections)
# More connections in pool = better parallel processing
spark.cdm.connect.target.yugabyte.pooling.maxSize=500

# Minimum pool size (increased to 100 connections)
# Maintains minimum connections for immediate availability
spark.cdm.connect.target.yugabyte.pooling.minSize=100

# Prepared statement cache size (increased to 2000)
# Caching prepared statements reduces query preparation time
spark.cdm.connect.target.yugabyte.preparedStatementCacheSize=2000

# =============================================================================
# SERIALIZATION OPTIMIZATION (FASTER DATA PROCESSING)
# =============================================================================
# Use Kryo serializer for better performance than default Java serialization
# Kryo is faster and more efficient for large datasets
spark.serializer=org.apache.spark.serializer.KryoSerializer

# Enable unsafe Kryo operations for maximum performance
# Unsafe operations are faster but require more careful handling
spark.kryo.unsafe=true

# Disable registration requirement for flexibility
# Allows dynamic class registration during runtime
spark.kryo.registrationRequired=false

# Maximum buffer size for Kryo serialization (64MB)
# Larger buffer = fewer serialization operations = better performance
spark.kryo.maxBufferSize=64m

# =============================================================================
# ERROR HANDLING (RESILIENCE)
# =============================================================================
# Maximum retry attempts for failed operations (reduced to 2 for speed)
# Fewer retries = faster failure detection = quicker recovery
spark.cdm.retry.maxAttempts=2

# Delay between retry attempts in milliseconds (reduced to 500ms)
# Shorter delay = faster retry = better performance
spark.cdm.retry.delayMs=500

# Continue processing even if some records fail
# Prevents entire migration from stopping due to individual record failures
spark.cdm.continueOnError=true

# =============================================================================
# LOGGING CONFIGURATION (PERFORMANCE MONITORING)
# =============================================================================
# Directory for migration logs and performance reports
# All performance metrics and failed records will be saved here
spark.cdm.log.directory=migration_logs

# Use quiet logging to reduce console output
# Focuses on important messages rather than verbose debug information
spark.driver.extraJavaOptions=-Dlog4j.configuration=file:log4j2-quiet.properties
spark.executor.extraJavaOptions=-Dlog4j.configuration=file:log4j2-quiet.properties

# =============================================================================
# CONSISTENCY LEVELS (DATA INTEGRITY)
# =============================================================================
# Read consistency from Cassandra (LOCAL_QUORUM for performance)
# LOCAL_QUORUM provides good performance while maintaining data consistency
spark.cdm.perfops.consistency.read=LOCAL_QUORUM

# Write consistency to YugabyteDB (LOCAL_QUORUM for performance)
# LOCAL_QUORUM balances performance with data consistency
spark.cdm.perfops.consistency.write=LOCAL_QUORUM

# =============================================================================
# ADDITIONAL PERFORMANCE PARAMETERS
# =============================================================================
# Disable speculative execution to prevent duplicate work
# Speculative execution can cause duplicate processing in some cases
spark.speculation=false

# Increase driver and executor memory fractions
# Allocates more memory for data processing operations
spark.executor.memoryFraction=0.8
spark.driver.memoryFraction=0.8

# Enable dynamic allocation for better resource utilization
# Automatically adjusts executor count based on workload
spark.dynamicAllocation.enabled=true
spark.dynamicAllocation.minExecutors=5
spark.dynamicAllocation.maxExecutors=20

# =============================================================================
# PERFORMANCE MONITORING ENABLEMENT
# =============================================================================
# Enable detailed performance logging
# This will create detailed performance reports showing:
# - Batch processing times
# - Throughput per batch
# - Resource utilization
# - Error rates
# - Overall migration performance
spark.cdm.performance.monitoring.enabled=true
spark.cdm.performance.monitoring.interval=30s
