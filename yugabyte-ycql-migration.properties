# =============================================================================
# CASSANDRA TO YUGABYTE YCQL MIGRATION CONFIGURATION
# =============================================================================
# This configuration migrates data from DataStax Cassandra to YugabyteDB YCQL
# YCQL uses the same CQL protocol as Cassandra, so we use standard Cassandra-to-Cassandra properties
#
# IMPORTANT: Use this file with the Migrate class (standard Cassandra migration)
# Command: --class com.datastax.cdm.job.Migrate
#
# The same DataStax driver is used for both connections:
# - CDM uses Spark Cassandra Connector (which uses DataStax Java Driver 4.19.0)
# - Both origin (Cassandra) and target (Yugabyte YCQL) use the same CassandraConnector
# - Yugabyte YCQL is fully Cassandra-compatible, so no special driver is needed
# =============================================================================

# =============================================================================
# CASSANDRA SOURCE CONFIGURATION
# =============================================================================
spark.cdm.connect.origin.host=vcausc11udev057.azr.bank-dns.com
spark.cdm.connect.origin.port=9042
spark.cdm.connect.origin.username=cassandraappiduat
spark.cdm.connect.origin.password=Uat@123456
spark.cdm.connect.origin.localDC=datacenter1
spark.cdm.connect.origin.consistencyLevel=LOCAL_ONE

# =============================================================================
# YUGABYTE YCQL TARGET CONFIGURATION
# =============================================================================
# YCQL uses port 9042 (same as Cassandra CQL port)
# Use standard target properties (NOT yugabyte-specific properties)
# Note: These use "target" prefix, NOT "target.yugabyte" prefix
spark.cdm.connect.target.host=vcausc11udev057.azr.bank-dns.com
spark.cdm.connect.target.port=9042
spark.cdm.connect.target.username=yugabyte
spark.cdm.connect.target.password=password
# Optional: Set local datacenter if needed
# spark.cdm.connect.target.localDC=datacenter1
# spark.cdm.connect.target.consistencyLevel=LOCAL_ONE

# =============================================================================
# SCHEMA CONFIGURATION
# =============================================================================
spark.cdm.schema.origin.keyspaceTable=customer_datastore.customer_mtrc_by_lpid
spark.cdm.schema.target.keyspaceTable=customer_datastore.customer_mtrc_by_lpid

# =============================================================================
# SPARK CONFIGURATION
# =============================================================================
# Resource Allocation Parameters:
# ------------------------------
# Number of CPU cores per executor (worker node)
# - Controls parallelism within each executor
# - Higher values = more parallel tasks per executor
# - Recommended: 2-8 cores per executor (depends on workload)
spark.executor.cores=4

# Number of executor instances (worker processes)
# - Total executors = instances × cores = 4 × 4 = 16 parallel tasks
# - For local mode: typically 1-4 instances
# - For cluster mode: adjust based on cluster size
spark.executor.instances=4

# Memory allocated to each executor (heap size)
# - Used for data caching, shuffles, and task execution
# - Leave ~10-20% for overhead (OS, JVM, etc.)
# - Example: 6G executor memory = ~5G usable for Spark
spark.executor.memory=6G

# Memory allocated to the Spark driver (master/coordinator)
# - Driver coordinates tasks, collects results, manages metadata
# - Needs more memory if collecting large results or broadcasting large tables
# - For local mode: should match or exceed executor memory
spark.driver.memory=6G

# Adaptive Query Execution (AQE) Parameters:
# -----------------------------------------
# Enable Adaptive Query Execution (Spark 3.0+)
# - Dynamically optimizes query execution based on runtime statistics
# - Automatically adjusts join strategies, partition sizes, etc.
# - Recommended: Always enable for better performance
spark.sql.adaptive.enabled=true

# Enable adaptive partition coalescing
# - Combines small partitions after shuffle operations
# - Reduces overhead from too many small partitions
# - Improves performance when partitions are unevenly sized
spark.sql.adaptive.coalescePartitions.enabled=true

# Enable adaptive skew join handling
# - Detects and handles data skew in joins automatically
# - Splits large partitions to balance workload
# - Critical for joins with uneven data distribution
spark.sql.adaptive.skewJoin.enabled=true

# Serialization Parameters:
# -------------------------
# Use Kryo serializer instead of default Java serializer
# - Kryo is faster and more efficient (2-10x faster)
# - Produces smaller serialized data (better network transfer)
# - Recommended: Always use Kryo for better performance
spark.serializer=org.apache.spark.serializer.KryoSerializer

# Adaptive Execution Tuning:
# ---------------------------
# Target partition size for adaptive execution
# - AQE tries to create partitions of this size
# - Smaller = more partitions = more parallelism but more overhead
# - Larger = fewer partitions = less overhead but less parallelism
# - Recommended: 32-128MB depending on data size
spark.sql.adaptive.advisoryPartitionSizeInBytes=32MB

# Timeout Parameters:
# -------------------
# Network timeout for all network operations
# - Time to wait for network I/O (fetching data, shuffles, etc.)
# - Increase if you have slow network or large data transfers
# - Default: 120s, but 600s (10 min) is safer for large migrations
spark.network.timeout=600s

# Broadcast join timeout
# - Time to wait for broadcast tables to be distributed to executors
# - Increase if broadcasting large tables (>100MB)
# - Should be >= network.timeout for large broadcasts
spark.sql.broadcastTimeout=600s

# =============================================================================
# CDM PERFORMANCE CONFIGURATION
# =============================================================================
spark.cdm.perfops.numParts=400
spark.cdm.perfops.ratelimit.origin=5000
spark.cdm.perfops.ratelimit.target=5000
spark.cdm.perfops.batchSize=5000
spark.cdm.perfops.fetchSizeInRows=5000

# =============================================================================
# CONNECTION SETTINGS
# =============================================================================
spark.cdm.connect.origin.timeout=120000
spark.cdm.connect.target.timeout=120000

# Spark Cassandra Connector connection settings (for troubleshooting connection issues)
# These settings help with "Lost connection to remote peer" and "Could not reach any contact point" errors
spark.cassandra.connection.timeout.ms=30000
spark.cassandra.connection.keep_alive_ms=30000
spark.cassandra.connection.local_dc=datacenter1
spark.cassandra.connection.reconnection_delay_ms.min=1000
spark.cassandra.connection.reconnection_delay_ms.max=60000
spark.cassandra.connection.max_requests_per_connection.local=32768
spark.cassandra.connection.max_requests_per_connection.remote=2000
spark.cassandra.connection.factory=com.datastax.spark.connector.cql.DefaultConnectionFactory

# =============================================================================
# SSL/TLS CONFIGURATION (Optional - Disabled by Default)
# =============================================================================
# SSL is DISABLED by default. To bypass SSL, leave these properties commented out.
# If you need to enable SSL, uncomment and configure the properties below.

# Origin (Cassandra) SSL Configuration
# spark.cdm.connect.origin.tls.enabled=false
# spark.cdm.connect.origin.tls.trustStore.path=
# spark.cdm.connect.origin.tls.trustStore.password=
# spark.cdm.connect.origin.tls.trustStore.type=JKS
# spark.cdm.connect.origin.tls.keyStore.path=
# spark.cdm.connect.origin.tls.keyStore.password=
# spark.cdm.connect.origin.tls.enabledAlgorithms=TLS_RSA_WITH_AES_128_CBC_SHA,TLS_RSA_WITH_AES_256_CBC_SHA

# Target (Yugabyte YCQL) SSL Configuration
# Quick setup for truststore.jks:
#   1. Get root certificate: scp user@yugabyte-node:/opt/yugabyte/tls/certs/ca.crt ./ca.crt
#   2. Create truststore: keytool -importcert -alias yugabyte-root-ca -file ca.crt \
#      -keystore truststore.jks -storetype JKS -storepass YourPassword -noprompt
#   3. Uncomment and configure the properties below
#
# spark.cdm.connect.target.tls.enabled=true
# spark.cdm.connect.target.tls.trustStore.path=/path/to/truststore.jks
# spark.cdm.connect.target.tls.trustStore.password=YourTruststorePassword
# spark.cdm.connect.target.tls.trustStore.type=JKS
# spark.cdm.connect.target.tls.keyStore.path=
# spark.cdm.connect.target.tls.keyStore.password=
# spark.cdm.connect.target.tls.enabledAlgorithms=TLS_RSA_WITH_AES_128_CBC_SHA,TLS_RSA_WITH_AES_256_CBC_SHA

# =============================================================================
# AUTOCORRECT AND TRACKING
# =============================================================================
spark.cdm.autocorrect.missing=false
spark.cdm.autocorrect.mismatch=false
spark.cdm.trackRun=false

# =============================================================================
# LOGGING
# =============================================================================
spark.cdm.log.directory=migration_logs
spark.cdm.log.level=INFO

