# =============================================================================
# CASSANDRA TO YUGABYTE YCQL - HIGH PERFORMANCE CONFIGURATION
# =============================================================================
# Optimized for: 4.5M rows, target throughput 10,000 reads/sec, 10,000 writes/sec
# Based on: dsbulk performance (9,500 reads/sec, 18,500 writes/sec proven)
# Expected: ~5-7 mins (vs 37 mins with default settings)
# =============================================================================

# =============================================================================
# CASSANDRA SOURCE CONFIGURATION
# =============================================================================
spark.cdm.connect.origin.host=vcausc11udev057.azr.bank-dns.com
spark.cdm.connect.origin.port=9042
spark.cdm.connect.origin.username=cassandraappiduat
spark.cdm.connect.origin.password=Uat@123456
spark.cdm.connect.origin.localDC=datacenter1
spark.cdm.connect.origin.consistencyLevel=LOCAL_ONE

# =============================================================================
# YUGABYTE YCQL TARGET CONFIGURATION
# =============================================================================
spark.cdm.connect.target.host=vcausc11udev057.azr.bank-dns.com
spark.cdm.connect.target.port=9042
spark.cdm.connect.target.username=yugabyte
spark.cdm.connect.target.password=password
spark.cdm.connect.target.localDC=datacenter1
spark.cdm.connect.target.consistencyLevel=LOCAL_ONE

# =============================================================================
# SCHEMA CONFIGURATION
# =============================================================================
spark.cdm.schema.origin.keyspaceTable=customer_datastore.customer_mtrc_by_lpid
spark.cdm.schema.target.keyspaceTable=customer_datastore.customer_mtrc_by_lpid

# =============================================================================
# SPARK CONFIGURATION
# =============================================================================
spark.executor.cores=4
spark.executor.instances=4
spark.executor.memory=6G
spark.driver.memory=6G

# Adaptive Query Execution
spark.sql.adaptive.enabled=true
spark.sql.adaptive.coalescePartitions.enabled=true
spark.sql.adaptive.skewJoin.enabled=true
spark.sql.adaptive.advisoryPartitionSizeInBytes=32MB

# Serialization
spark.serializer=org.apache.spark.serializer.KryoSerializer

# Timeouts
spark.network.timeout=600s
spark.sql.broadcastTimeout=600s

# =============================================================================
# CDM PERFORMANCE CONFIGURATION - HIGH THROUGHPUT
# =============================================================================
# Target: 10,000 reads/sec, 10,000 writes/sec
# Proven capabilities: Origin 9,500/sec, Target 18,500/sec (via dsbulk)

# Number of partitions - AGGRESSIVE for maximum parallelism
# - 5000 partitions = ~900 rows per partition (4.5M / 5000)
# - Each partition processed in parallel
# - Higher numParts = more parallelism, but needs more connections
spark.cdm.perfops.numParts=5000

# Rate limits - MATCH PROVEN CAPABILITIES
# - Origin proven: 9,500/sec (dsbulk with rate limit 10,000)
# - Target proven: 18,500/sec (dsbulk without rate limit)
# - Setting both to 10,000 to match origin capability
spark.cdm.perfops.ratelimit.origin=10000
spark.cdm.perfops.ratelimit.target=10000

# Batch size - Optimized for small-medium rows
# - Large batches = fewer network round-trips = better throughput
# - 5000 records per batch is good for small rows (< 1KB)
spark.cdm.perfops.batchSize=5000

# Fetch size - Optimized for small-medium rows
# - Large fetch = fewer queries = better throughput
# - 5000 rows per fetch is good for small rows (< 1KB)
spark.cdm.perfops.fetchSizeInRows=5000

# =============================================================================
# CONNECTION SETTINGS - CRITICAL FOR HIGH PARALLELISM
# =============================================================================
spark.cdm.connect.origin.timeout=120000
spark.cdm.connect.target.timeout=120000

# =============================================================================
# SPARK CASSANDRA CONNECTOR - CONNECTION POOL (CRITICAL!)
# =============================================================================
# For numParts=5000 and rate limits=10000, you need MORE connections per executor
# 
# Connection Pool Calculation:
# - 4 executors × 4 cores × 4 connections = 64 connections
# - This should be within YugabyteDB limits (~100-200 per node)
#
# If you see "no connection" errors:
# 1. Check YugabyteDB connection limits
# 2. Reduce connections_per_executor_local to 3
# 3. Or reduce numParts to 2000-3000

# Connections per executor - INCREASED for high parallelism
# - Default: 1 (too low - causes "no connection" errors!)
# - For numParts=5000: Use 4 connections per executor
# NOTE: Property names are camelCase, not snake_case!
spark.cassandra.connection.localConnectionsPerExecutor=4
spark.cassandra.connection.remoteConnectionsPerExecutor=1

# Connection timeout - INCREASED for high parallelism
# - Default: 5000ms (too short!)
# - For high parallelism: 60000ms (60 seconds)
spark.cassandra.connection.timeout.ms=60000
spark.cassandra.connection.keep_alive_ms=30000

# Local datacenter
spark.cassandra.connection.local_dc=datacenter1

# Reconnection settings
spark.cassandra.connection.reconnection_delay_ms.min=1000
spark.cassandra.connection.reconnection_delay_ms.max=60000

# Max requests per connection
spark.cassandra.connection.max_requests_per_connection.local=32768
spark.cassandra.connection.max_requests_per_connection.remote=2000
spark.cassandra.connection.factory=com.datastax.spark.connector.cql.DefaultConnectionFactory

# =============================================================================
# AUTOCORRECT AND TRACKING
# =============================================================================
spark.cdm.autocorrect.missing=false
spark.cdm.autocorrect.mismatch=false
spark.cdm.trackRun=false

# =============================================================================
# LOGGING
# =============================================================================
spark.cdm.log.directory=migration_logs
spark.cdm.log.level=INFO

# =============================================================================
# GRADUAL SCALING RECOMMENDATION
# =============================================================================
# If this configuration fails with connection errors, scale gradually:
#
# STEP 1: Start with numParts=1000, connections_per_executor_local=2
#   - spark.cdm.perfops.numParts=1000
#   - spark.cdm.perfops.ratelimit.origin=5000
#   - spark.cdm.perfops.ratelimit.target=5000
#   - spark.cassandra.connection.connections_per_executor_local=2
#   - Expected: ~15-20 mins
#
# STEP 2: If successful, try numParts=2000, connections_per_executor_local=3
#   - spark.cdm.perfops.numParts=2000
#   - spark.cdm.perfops.ratelimit.origin=7500
#   - spark.cdm.perfops.ratelimit.target=7500
#   - spark.cassandra.connection.connections_per_executor_local=3
#   - Expected: ~10-12 mins
#
# STEP 3: If successful, try numParts=5000, connections_per_executor_local=4 (current)
#   - Expected: ~5-7 mins
#
# If connection errors persist:
# 1. Check YugabyteDB connection limits
# 2. Reduce connections_per_executor_local
# 3. Reduce numParts
# 4. Check YugabyteDB cluster health and resources

