# Rate Limit Configuration Analysis

## Summary

The rate limits shown in the migration summary (1000 reads/sec, 1000 writes/sec) are **INCORRECT** and appear to be a bug in the summary generation code. The **actual rate limiters** are correctly initialized with **20,000** (the default value).

## Evidence

### 1. Actual Rate Limiters (CORRECT)
From migration logs:
```
25/12/18 22:58:22 INFO YugabyteCopyJobSession: PARAM -- Origin Rate Limit: 20000.0
25/12/18 22:58:22 INFO YugabyteCopyJobSession: PARAM -- Target Rate Limit: 20000.0
```

**Location:** `src/main/java/com/datastax/cdm/job/AbstractJobSession.java:58-59`
```java
rateLimiterOrigin = RateLimiter.create(propertyHelper.getInteger(KnownProperties.PERF_RATELIMIT_ORIGIN));
rateLimiterTarget = RateLimiter.create(propertyHelper.getInteger(KnownProperties.PERF_RATELIMIT_TARGET));
```

### 2. Migration Summary (INCORRECT)
From `migration_logs/migration_summary_20251218_225822.txt`:
```
CDM Configuration:
  Origin Rate Limit: 1000 reads/sec
  Target Rate Limit: 1000 writes/sec
```

**Location:** `src/main/scala/com/datastax/cdm/job/YugabyteMigrate.scala:102-103`
```scala
val originRateLimit = propertyHelper.getInteger(KnownProperties.PERF_RATELIMIT_ORIGIN);
val targetRateLimit = propertyHelper.getInteger(KnownProperties.PERF_RATELIMIT_TARGET);
```

## Root Cause

The migration summary is generated by `buildConfigurationSummary()` which reads from `propertyHelper.getInteger()`. However, when this method is called, it appears to be returning `null` or an incorrect value, causing the summary to display hardcoded defaults (1000) instead of the actual configured values (20000).

## Properties File Configuration

### Current State in `transaction-test-audit.properties`:
```properties
# Rate limits - set high or comment out to disable
# For maximum throughput, comment out these lines
# spark.cdm.perfops.ratelimit.origin=50000
# spark.cdm.perfops.ratelimit.target=50000
```

**Status:** Rate limit properties are **COMMENTED OUT**, so the system uses **default values** (20,000).

### Default Values (from KnownProperties.java):
```java
defaults.put(PERF_RATELIMIT_ORIGIN, "20000");
defaults.put(PERF_RATELIMIT_TARGET, "20000");
```

## How Properties Are Loaded

### 1. Property Loading Flow:
1. **Spark loads properties file** via `--properties-file transaction-test-audit.properties`
2. **PropertyHelper.loadSparkConf()** processes all SparkConf properties
3. **Known properties** are validated and stored in `propertyMap`
4. **Missing properties** get default values from `KnownProperties.defaults`
5. **Rate limiters** are initialized using `propertyHelper.getInteger()`

### 2. Code Path:
```
SparkSubmit
  → SparkConf.loadFromPropertiesFile()
  → PropertyHelper.getInstance(SparkConf)
  → PropertyHelper.loadSparkConf()
  → AbstractJobSession constructor
  → RateLimiter.create(propertyHelper.getInteger(...))
```

## Verification

### Properties Actually Loaded:
From migration logs:
```
25/12/18 22:58:21 INFO PropertyHelper: Known property [spark.cdm.perfops.batchSize] is configured with value [25]
25/12/18 22:58:21 INFO PropertyHelper: Known property [spark.cdm.perfops.numParts] is configured with value [40]
25/12/18 22:58:21 INFO PropertyHelper: Known property [spark.cdm.connect.target.yugabyte.batchSize] is configured with value [50]
```

**Note:** Rate limit properties are NOT logged, which means they're using defaults (20000).

## Conclusion

1. ✅ **Rate limiters are correctly initialized** with 20,000 (default value)
2. ✅ **Properties file is being read** (other properties like batchSize, numParts are loaded)
3. ❌ **Migration summary has a bug** - it shows incorrect values (1000 instead of 20000)
4. ✅ **Actual migration performance** is NOT limited by 1000 - it's using 20,000 rate limit

## Impact on Performance

The **actual throughput achieved (3,159 IOPS)** is NOT limited by the 1000 value shown in the summary. The real bottleneck is likely:
- Network latency
- Database write performance
- Spark parallelism configuration
- Batch processing overhead

The rate limiters are set to 20,000, which is well above the achieved throughput.

## Recommendations

1. **Fix the migration summary bug** - The `buildConfigurationSummary()` method should correctly read rate limit values
2. **To achieve 10K-20K IOPS**, consider:
   - Removing rate limits entirely (comment out in properties file)
   - Increasing Spark parallelism
   - Optimizing batch sizes
   - Checking network/database performance

